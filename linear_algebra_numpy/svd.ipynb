{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bigger-photograph",
   "metadata": {},
   "source": [
    "奇异值分解(Singular Value Decomposition，以下简称SVD)是在机器学习领域广泛应用的算法，它不光可以用于降维算法中的特征分解，还可以用于推荐系统，以及自然语言处理等领域。是很多机器学习算法的基石。本文就对SVD的原理做一个总结，并讨论在在PCA降维算法中是如何运用运用SVD的。\n",
    "\n",
    "1. 回顾特征值和特征向量\n",
    "　　　　我们首先回顾下特征值和特征向量的定义如下：\n",
    "𝐴𝑥=𝜆𝑥\n",
    "　　　　其中A是一个𝑛×𝑛的实对称矩阵，𝑥是一个𝑛维向量，则我们说𝜆是矩阵A的一个特征值，而𝑥是矩阵A的特征值𝜆所对应的特征向量。\n",
    "\n",
    "　　　　求出特征值和特征向量有什么好处呢？ 就是我们可以将矩阵A特征分解。如果我们求出了矩阵A的𝑛个特征值𝜆1≤𝜆2≤...≤𝜆𝑛,以及这𝑛个特征值所对应的特征向量{𝑤1,𝑤2,...𝑤𝑛}，，如果这𝑛个特征向量线性无关，那么矩阵A就可以用下式的特征分解表示：𝐴=𝑊Σ𝑊^−1\n",
    "　　　　其中W是这𝑛个特征向量所张成的𝑛×𝑛维矩阵，而Σ为这n个特征值为主对角线的𝑛×𝑛维矩阵。\n",
    "\n",
    "　　　　一般我们会把W的这𝑛个特征向量标准化，即满足||𝑤𝑖||2=1, 或者说𝑤𝑇𝑖𝑤𝑖=1，此时W的𝑛个特征向量为标准正交基，满足𝑊𝑇𝑊=𝐼，即𝑊𝑇=𝑊−1, 也就是说W为酉矩阵。\n",
    "\n",
    "　　　　这样我们的特征分解表达式可以写成\n",
    "    \n",
    "    𝐴=𝑊Σ𝑊𝑇\n",
    " \n",
    "　　　　注意到要进行特征分解，矩阵A必须为方阵。那么如果A不是方阵，即行和列不相同时，我们还可以对矩阵进行分解吗？答案是可以，此时我们的SVD登场了。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "binding-vector",
   "metadata": {},
   "source": [
    "2.  SVD的定义\n",
    "　　　　SVD也是对矩阵进行分解，但是和特征分解不同，SVD并不要求要分解的矩阵为方阵。假设我们的矩阵A是一个𝑚×𝑛的矩阵，那么我们定义矩阵A的SVD为：\n",
    "𝐴=𝑈Σ𝑉𝑇\n",
    "　　　　其中U是一个𝑚×𝑚的矩阵，Σ是一个𝑚×𝑛的矩阵，除了主对角线上的元素以外全为0，主对角线上的每个元素都称为奇异值，V是一个𝑛×𝑛的矩阵。U和V都是酉矩阵，即满足𝑈𝑇𝑈=𝐼,𝑉𝑇𝑉=𝐼。\n",
    "    \n",
    "    那么我们如何求出SVD分解后的𝑈,Σ,𝑉这三个矩阵呢？\n",
    "\n",
    "　　　　如果我们将A的转置和A做矩阵乘法，那么会得到𝑛×𝑛的一个方阵𝐴𝑇𝐴。既然𝐴𝑇𝐴是方阵，那么我们就可以进行特征分解，得到的特征值和特征向量满足下式：\n",
    "(𝐴𝑇𝐴)𝑣𝑖=𝜆𝑖𝑣𝑖\n",
    "　　　　这样我们就可以得到矩阵𝐴𝑇𝐴的n个特征值和对应的n个特征向量𝑣了。将𝐴𝑇𝐴的所有特征向量张成一个𝑛×𝑛的矩阵V，就是我们SVD公式里面的V矩阵了。一般我们将V中的每个特征向量叫做A的右奇异向量。\n",
    "\n",
    "　　　　如果我们将A和A的转置做矩阵乘法，那么会得到𝑚×𝑚的一个方阵𝐴𝐴𝑇。既然𝐴𝐴𝑇是方阵，那么我们就可以进行特征分解，得到的特征值和特征向量满足下式：\n",
    "(𝐴𝐴𝑇)𝑢𝑖=𝜆𝑖𝑢𝑖\n",
    "　　　　这样我们就可以得到矩阵𝐴𝐴𝑇的m个特征值和对应的m个特征向量𝑢了。将𝐴𝐴𝑇的所有特征向量张成一个𝑚×𝑚的矩阵U，就是我们SVD公式里面的U矩阵了。一般我们将U中的每个特征向量叫做A的左奇异向量。\n",
    "\n",
    "　　　　U和V我们都求出来了，现在就剩下奇异值矩阵Σ没有求出了。由于Σ除了对角线上是奇异值其他位置都是0，那我们只需要求出每个奇异值𝜎就可以了。\n",
    "\n",
    "　　　　我们注意到:\n",
    "𝐴=𝑈Σ𝑉𝑇⇒𝐴𝑉=𝑈Σ𝑉𝑇𝑉⇒𝐴𝑉=𝑈Σ⇒𝐴𝑣𝑖=𝜎𝑖𝑢𝑖⇒𝜎𝑖=𝐴𝑣𝑖/𝑢𝑖\n",
    " 　　　 这样我们可以求出我们的每个奇异值，进而求出奇异值矩阵Σ。\n",
    "\n",
    "　　　 上面还有一个问题没有讲，就是我们说𝐴𝑇𝐴的特征向量组成的就是我们SVD中的V矩阵，而𝐴𝐴𝑇的特征向量组成的就是我们SVD中的U矩阵，这有什么根据吗？这个其实很容易证明，我们以V矩阵的证明为例。\n",
    "𝐴=𝑈Σ𝑉𝑇⇒𝐴𝑇=𝑉Σ𝑇𝑈𝑇⇒𝐴𝑇𝐴=𝑉Σ𝑇𝑈𝑇𝑈Σ𝑉𝑇=𝑉Σ2𝑉𝑇\n",
    "　　　　上式证明使用了:𝑈𝑇𝑈=𝐼,Σ𝑇Σ=Σ2。可以看出𝐴𝑇𝐴的特征向量组成的的确就是我们SVD中的V矩阵。类似的方法可以得到𝐴𝐴𝑇的特征向量组成的就是我们SVD中的U矩阵。\n",
    "\n",
    "　　　　进一步我们还可以看出我们的特征值矩阵等于奇异值矩阵的平方，也就是说特征值和奇异值满足如下关系：\n",
    "𝜎𝑖=𝜆𝑖‾‾√\n",
    "　　　　这样也就是说，我们可以不用𝜎𝑖=𝐴𝑣𝑖/𝑢𝑖来计算奇异值，也可以通过求出𝐴𝑇𝐴的特征值取平方根来求奇异值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "welcome-dancing",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Models\n",
    "* One traditional approach\n",
    "* And one deep approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a Language Model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A model which tries to assess the liklehood of language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$P(W) = P(w_1, w_2, ..., w_n)$\n",
    "\n",
    "or\n",
    "\n",
    "$P(w_{t+1} | w_{t-1+n}, ..., w_{t})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Three main areas that are more or less 'likely' to occur:\n",
    "* Syntax issues - e.g. I go home vs I home go\n",
    "* Semantic issues - I go home vs I go house\n",
    "* Pragmatics issues - I go home vs I go home and 2+2=4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All this knowledge neds to be captured inside the pairings of words with other words!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A 'traditional' language model... for Sequence Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A bigram Markov chain (more later in the course)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigram_word_distribution(data):\n",
    "    \"\"\"create a probability distribution over all bigrams\n",
    "    \n",
    "        params: data - a Bunch data object from sklearn\n",
    "        returns: Word probability distribution\n",
    "    \"\"\"\n",
    "\n",
    "    text = data['data']\n",
    "    all_data = ' '.join([' '.join(re.findall('(?u)\\\\b\\\\w\\\\w*\\\\b',article.lower())) for article in text]).split()\n",
    "    words = pd.DataFrame({'words':all_data})\n",
    "    words['next_words'] = words['words'].shift(-1)\n",
    "    word_distribution = words.groupby('words')['next_words'].value_counts(normalize=True)\n",
    "    \n",
    "    return word_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigram_text_generation(seed, length, distribution):\n",
    "    \"\"\"seed a distribution with a seed word, and ask it to make more words\n",
    "        \n",
    "        params: seed - A seed word, \n",
    "                length -Length of the generated sentence\n",
    "                distribution - A word probability distribution\n",
    "                \n",
    "        returns: generated sentence\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        seed = seed.lower()\n",
    "        for i in range(length):\n",
    "             seed += ' ' + np.random.choice(distribution[seed.split()[-1]].index, p=distribution[seed.split()[-1]].values)\n",
    "        return seed\n",
    "    \n",
    "    except:\n",
    "        print('Oops! Try another seed')\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = fetch_20newsgroups(remove=['headers', 'footers'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'filenames', 'target_names', 'target', 'DESCR'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([\"A fair number of brave souls who upgraded their SI clock oscillator have\\nshared their experiences for this poll. Please send a brief message detailing\\nyour experiences with the procedure. Top speed attained, CPU rated speed,\\nadd on cards and adapters, heat sinks, hour of usage per day, floppy disk\\nfunctionality with 800 and 1.4 m floppies are especially requested.\\n\\nI will be summarizing in the next two days, so please add to the network\\nknowledge base if you have done the clock upgrade and haven't answered this\\npoll. Thanks.\"],\n",
       " 11314)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['data'][1:2], len(data['data'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the bigram probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_dist = bigram_word_distribution(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 'It'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_bigram = bigram_text_generation(seed, 20, bi_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'it is removed from the disk doctor i build say about this sounds very little rom s immune system and feel'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_bigram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How can we improve it?\n",
    "* We're using bigram predictions, instead we can use trigram\n",
    "* Take context from the previous 2 words instead of the previous word only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trigram_word_distribution(data):\n",
    "    \"\"\"create a probability distribution over all trigrams\n",
    "    \n",
    "        params: data - a Bunch data object from sklearn\n",
    "        returns: [Bigram probability distribution, trigram probability distribution]\n",
    "    \"\"\"\n",
    "    \n",
    "    text = data['data']\n",
    "    all_data = ' '.join([' '.join(re.findall('(?u)\\\\b\\\\w\\\\w*\\\\b',article.lower())) for article in text]).split()\n",
    "    tri_gram = [' '.join([x,y]) for x,y in zip(all_data[:-1:], all_data[1::])]\n",
    "    next_word = all_data[2:] + [' '] * 1\n",
    "    words = pd.DataFrame({'seed_word':all_data[:-1],'gram_words':tri_gram, \"next_word\":next_word})\n",
    "    words['seed_next_word'] = words['seed_word'].shift(-1)\n",
    "    seed_word_distribution = words.groupby('seed_word')['seed_next_word'].value_counts(normalize=True)\n",
    "    gram_word_distribution = words.groupby('gram_words')['next_word'].value_counts(normalize=True)\n",
    "    \n",
    "    return [seed_word_distribution, gram_word_distribution]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tri_dist = trigram_word_distribution(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trigram_text_generation(seed, length, distribution):\n",
    "    \"\"\"seed a distribution with a seed word, and ask it to make more words\n",
    "        \n",
    "        params: seed - A seed word, \n",
    "                length -Length of the generated sentence\n",
    "                distribution - A word probability distribution\n",
    "                \n",
    "        returns: generated sentence\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        seed = seed.lower()\n",
    "        seed += ' ' + np.random.choice(distribution[0][seed].index, p=distribution[0][seed].values)\n",
    "        for i in range(length):\n",
    "             seed += ' ' + np.random.choice(distribution[1][' '.join(seed.split()[-2:])].index, p=distribution[1][' '.join(seed.split()[-2:])].values)\n",
    "        return seed\n",
    "    \n",
    "    except:\n",
    "        print('Oops! Try another seed')\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_trigram = trigram_text_generation('Because',20,tri_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'because all the text itself b parallel passages my reply please be aware of the whosoever wont s whether slaves were discouraged'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_trigram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other areas\n",
    "* Monitor sentence start distributions differently\n",
    "* Add start and end tokens to generate separable sentences\n",
    "* Add other punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drawbacks:\n",
    "\n",
    "* This model typs requires lots of computation power to train, and a lot of space to store advanced models\n",
    "* N-grams are a sparse representation of language -  any word not present in the training corpus has a zero probability chance of being used\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Better approach - Deep Language Models!\n",
    "* Deep Language generation using LSTMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab the text\n",
    "text = data['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a continuous list of words\n",
    "all_data = ' '.join([' '.join(re.findall('(?u)\\\\b\\\\w\\\\w*\\\\b',article.lower())) for article in text]).split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# work out the vocab and size of vocab\n",
    "vocab_list = list(set(all_data))\n",
    "n_vocab = len(vocab_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#translate words to numbers\n",
    "word_to_num = {}\n",
    "num_to_word = {}\n",
    "for i, word in enumerate(vocab_list):\n",
    "    num_to_word[i] = word\n",
    "    word_to_num[vocab_list[i]] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embed the data\n",
    "embedded_data = [word_to_num[word] for word in all_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the next word guess for each previous 10 words\n",
    "X_data= []\n",
    "y_data = []\n",
    "seq_length=10\n",
    "for i in range(len(embedded_data)-seq_length):\n",
    "    X_data.append(embedded_data[i:i+seq_length])\n",
    "    y_data.append(embedded_data[i+seq_length])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape the X and y data\n",
    "X = np.array(X_data).reshape(len(X_data), seq_length, 1)\n",
    "y = to_categorical(y_data)\n",
    "\n",
    "#normalise the X data\n",
    "X = X / float(n_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save our best version of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'epoch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-b0047e9d4b6a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfilepath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf\"weights_{epoch:2d}.hdf5\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModelCheckpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_best_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'min'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcallbacks_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'epoch' is not defined"
     ]
    }
   ],
   "source": [
    "filepath=f\"weights_{epoch:2d}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model\n",
    "#history = model.fit(X, y, epochs=1, batch_size=128, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a saved model\n",
    "# filename = \"weights_01.hdf5\"\n",
    "# model.load_weights(filename)\n",
    "# model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For a given input string generate some new text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_input(seed_input):\n",
    "    \"\"\"prepare a string for the LSTM\"\"\"\n",
    "    \n",
    "    seed_input = seed_input.split()\n",
    "    try:\n",
    "        return np.expand_dims(np.array([word_to_num[x] for x in seed_input]).reshape(-1,1),axis=0)\n",
    "    except:\n",
    "        return 'please try with different words'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(input_string):\n",
    "    \"\"\"generate some new text as a string\"\"\"\n",
    "    \n",
    "    seed = prepare_input(seed)\n",
    "    for i in range(10):\n",
    "    #predict next word based on window of 10 previous words - and add to embedded doc\n",
    "        next_word = np.argmax(model.predict(seed[:,i:,:])).reshape(1,-1,1)\n",
    "        seed = np.append(seed,next_word,axis=1)\n",
    "\n",
    "    return ' '.join([num_to_word[x] for x in seed[0,:,0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2 \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.applications import mobilenet_v2\n",
    "from tensorflow.keras import preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning for Neural Network\n",
    "\n",
    "![](https://neurohive.io/wp-content/uploads/2018/11/vgg16-1-e1542731207177.png)\n",
    "\n",
    "> Transfer learning consists of taking features learned on one problem, and leveraging them on a new, similar problem. For instance, features from a model that has learned to identify racoons may be useful to kick-start a model meant to identify tanukis.\n",
    "\n",
    "\n",
    "1. Take the weights and architecture of a [pre-trained network](https://keras.io/api/applications/)\n",
    "2. Load the \"convolutional base\" of the model (everything except the final dense layers)\n",
    "3. Freeze all the layers of the base (weights become fixed)\n",
    "4. Add a fully connected dense layer on top\n",
    "5. Add a task specific dense output layer\n",
    "6. Compile and fit the model to your data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting image data into keras\n",
    "Keras has its own in build Objects and Methods to get image data in efficiently\n",
    "See: https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image\n",
    "\n",
    "- `class ImageDataGenerator`: Generate batches of tensor image data with real-time data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this are the folder names of the things you want to classify\n",
    "classes = ['phone', 'wallet']\n",
    "# plug in the path to your data folder\n",
    "base_path = 'data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define an image data generator\n",
    "data_gen = preprocessing.image.ImageDataGenerator(\n",
    "    # define the preprocessing function that should be applied to all images\n",
    "    preprocessing_function=mobilenet_v2.preprocess_input,\n",
    "    # fill_mode='nearest',\n",
    "    # rotation_range=20,\n",
    "    # width_shift_range=0.2,\n",
    "    # height_shift_range=0.2,\n",
    "    # horizontal_flip=True, \n",
    "    # zoom_range=0.2,\n",
    "    # shear_range=0.2    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 151 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# a generator that returns batches of X and y arrays\n",
    "train_data_gen = data_gen.flow_from_directory(\n",
    "        directory=base_path,\n",
    "        class_mode=\"categorical\",\n",
    "        classes=classes,\n",
    "        batch_size=135,\n",
    "        target_size=(224, 224)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((135, 224, 224, 3), (135, 2))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load in all images at once\n",
    "xtrain, ytrain = next(train_data_gen)\n",
    "xtrain.shape, ytrain.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (Advanced: Data augmentation)\n",
    "\n",
    "> https://keras.io/guides/transfer_learning/\n",
    "\n",
    "Applies random distortions and transformations to the images (only on your training data!). You need to store your training and validation data at separate locations and use a second `ImageDataGenerator` for your validation data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select the convolutional base and freeze the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = mobilenet_v2.MobileNetV2(\n",
    "    weights='imagenet', \n",
    "    alpha=0.35,         # specific parameter of this model, small alpha reduces the number of overall weights\n",
    "    pooling='avg',      # applies global average pooling to the output of the last conv layer (like a flattening)\n",
    "    include_top=False,  # we only want to have the base, not the final dense layers \n",
    "    input_shape=(224, 224, 3)\n",
    ")\n",
    "\n",
    "# freeze it!\n",
    "base_model.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add your own dense layers on top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "mobilenetv2_0.35_224 (Functi (None, 1280)              410208    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 100)               128100    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 202       \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 2)                 8         \n",
      "=================================================================\n",
      "Total params: 538,518\n",
      "Trainable params: 128,306\n",
      "Non-trainable params: 410,212\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(base_model)\n",
    "model.add(keras.layers.Dense(100, activation='relu'))\n",
    "model.add(keras.layers.Dropout(0.5))\n",
    "model.add(keras.layers.Dense(len(classes), activation='softmax'))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "# have a look at the trainable and non-trainable params statistic\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile and train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1/1 - 5s - loss: 9.6023 - categorical_accuracy: 0.4255 - val_loss: nan - val_categorical_accuracy: 0.4634\n",
      "Epoch 2/50\n",
      "1/1 - 1s - loss: nan - categorical_accuracy: 0.5745 - val_loss: nan - val_categorical_accuracy: 0.4634\n",
      "Epoch 3/50\n",
      "1/1 - 1s - loss: nan - categorical_accuracy: 0.5745 - val_loss: nan - val_categorical_accuracy: 0.4634\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f8d01635490>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "              loss=keras.losses.categorical_crossentropy,\n",
    "              metrics=[keras.metrics.categorical_accuracy])\n",
    "\n",
    "# observe the validation loss and stop when it does not improve after 3 iterations\n",
    "callback = keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
    "\n",
    "model.fit(xtrain, ytrain, \n",
    "          epochs=50, \n",
    "          verbose=2,\n",
    "          batch_size=len(xtrain), \n",
    "          callbacks=[callback],\n",
    "          # use 30% of the data for validation\n",
    "          validation_split=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Use it to predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing import image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = image.load_img('phone_paula.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = image.img_to_array(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.expand_dims(a, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(a)[0].round(decimals = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(x = classes, height = model.predict(a)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save your model for later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('models/wallet_phone.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### (Advanced Optional Step: Fine Tuning)\n",
    "\n",
    "This is done after the initial training! Adapt a few of the base layers to the specific learning task by retraining the model. This can improve accuracy, especially if the original learning task of the pre-trained model differs a lot from the actual task.\n",
    "\n",
    "1. Unfreeze some (or all) of the layers in the convolutional base (starting with the base output layer)\n",
    "2. Recompile your model and choose a very low learning rate (`1e-5`)\n",
    "2. Continue training the model but stop early to avoid overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to continue!\n",
    "\n",
    "- Load the trained model into `predict.py` (the modified `capture.py` with the `predict_frame(frame)` function) \n",
    "\n",
    "- If you don't have it yet, write a fuction `predict_frame(frame)` that uses the trained model to predict the object in the current frame. It should return a dictionary of class probabilities and names.\n",
    "    - make sure that the input image to the model is of size (224, 224)\n",
    "- Modify the script such that it makes a prediction once you press the `p` key\n",
    "- Write the prediction as a log message to the terminal\n",
    "\n",
    "### Advanced\n",
    "\n",
    "- Display the result of the prediction on the current webcam frame\n",
    "- Make an automatic prediction every second (Hint: the `while` loop has a speed of approx. 30 frames per second)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.engine.sequential.Sequential at 0x7f8d18c98130>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.models.load_model('models/wallet_phone.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learning",
   "language": "python",
   "name": "deep_learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
